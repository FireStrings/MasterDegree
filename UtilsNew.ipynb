{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FireStrings/MasterDegree/blob/main/UtilsNew.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b0b537ac-4cb6-4015-b700-3d8ffe7e1336",
      "metadata": {
        "id": "b0b537ac-4cb6-4015-b700-3d8ffe7e1336"
      },
      "source": [
        "### Libs import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a8efb8f7-2e74-4897-9d6a-950b9238505a",
      "metadata": {
        "id": "a8efb8f7-2e74-4897-9d6a-950b9238505a"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pandas.errors import SettingWithCopyWarning\n",
        "from pandas.plotting import autocorrelation_plot\n",
        "import seaborn as sns\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "\n",
        "from scipy.special import expit\n",
        "from scipy.stats import zscore\n",
        "from scipy import stats\n",
        "\n",
        "import os\n",
        "import io\n",
        "import math\n",
        "from os import listdir\n",
        "from os import system\n",
        "import subprocess\n",
        "from functools import reduce\n",
        "\n",
        "import statsmodels.tsa.stattools as ts\n",
        "import statsmodels.api as sm\n",
        "from statsmodels.tsa.stattools import adfuller\n",
        "from statsmodels.tsa.seasonal import seasonal_decompose\n",
        "\n",
        "from time import time\n",
        "from pytz import timezone, all_timezones_set, common_timezones_set\n",
        "import pytz\n",
        "\n",
        "import random\n",
        "import tensorflow as tf\n",
        "\n",
        "import pickle\n",
        "\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "warnings.simplefilter(action='ignore', category=UserWarning)\n",
        "warnings.simplefilter(action='ignore', category=SettingWithCopyWarning)\n",
        "\n",
        "\n",
        "%matplotlib inline\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b45b86e7-c1f5-446a-a8ec-cdb60bd40d00",
      "metadata": {
        "id": "b45b86e7-c1f5-446a-a8ec-cdb60bd40d00"
      },
      "source": [
        "### Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f454a7a6-7ece-410e-942d-7dac4a75418b",
      "metadata": {
        "id": "f454a7a6-7ece-410e-942d-7dac4a75418b"
      },
      "outputs": [],
      "source": [
        "def manage_results(_file, op, results=None):\n",
        "    if op == \"read\":\n",
        "        with open(_file, 'rb') as f:\n",
        "            loaded_dict = pickle.load(f)\n",
        "            return loaded_dict\n",
        "    else:\n",
        "        with open(_file, \"wb\") as f:\n",
        "            pickle.dump(results, f)\n",
        "\n",
        "def manage_models(_file, op, results=None):\n",
        "    if op == \"read\":\n",
        "        with open(_file, 'rb') as f:\n",
        "            loaded_model = pickle.load(f)\n",
        "            return loaded_model\n",
        "    else:\n",
        "        with open(_file, \"wb\") as f:\n",
        "            pickle.dump(results, f)\n",
        "\n",
        "def define_seed(seed):\n",
        "  np.random.seed(seed)\n",
        "  tf.random.set_seed(seed)\n",
        "  random.seed(seed)\n",
        "  tf.config.experimental.enable_op_determinism()\n",
        "\n",
        "def select_and_normalize_minmax(anual_df, list_cols):\n",
        "    df = anual_df[list_cols]\n",
        "\n",
        "    scaler = MinMaxScaler()\n",
        "    df_scaled = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)\n",
        "\n",
        "    return df_scaled.fillna(df_scaled.mean())\n",
        "\n",
        "def select_and_normalize_divided(anual_df, list_cols):\n",
        "\n",
        "    def divided_by_1000(n):\n",
        "        return n/1000\n",
        "\n",
        "    df = anual_df[list_cols]\n",
        "\n",
        "    df_scaled = df.apply(divided_by_1000)\n",
        "\n",
        "    return df_scaled.fillna(df_scaled.mean())\n",
        "\n",
        "def set_num_rows_cols(nr, nc):\n",
        "    pd.set_option('display.max_columns', nc)\n",
        "    pd.set_option('display.max_rows', nr)\n",
        "\n",
        "def set_size_plot(x, y):\n",
        "    sns.set_theme(rc={'figure.figsize':(x, y)})\n",
        "\n",
        "def getSigmoid():\n",
        "    arr = np.arange(0, 1, 0.02)\n",
        "    np.hstack(np.vstack(arr))\n",
        "    def sigmoid(x):\n",
        "        return 1/(1+expit(-x))\n",
        "\n",
        "    sigmoid(arr)\n",
        "    sns.scatterplot(x=arr, y=sigmoid(arr))\n",
        "\n",
        "def load_files(station, preproc=True):\n",
        "\n",
        "    raw_anual_df = pd.read_csv(\"data/\" + station + \".csv\", sep=\";\")\n",
        "\n",
        "    if preproc:\n",
        "        anual_df = pre_processing(raw_anual_df)\n",
        "\n",
        "        anual_df[\"hora\"] = anual_df[\"data_hora\"].dt.hour\n",
        "    else:\n",
        "        return raw_anual_df\n",
        "\n",
        "    return anual_df\n",
        "\n",
        "def load_and_filter(station, start_hour, end_hour):\n",
        "\n",
        "    anual_df = load_files(station)\n",
        "\n",
        "    anual_df = filter_between(anual_df, \"hora\", start_hour, end_hour)\n",
        "\n",
        "    return anual_df\n",
        "\n",
        "def data_to_input_and_output_lstm(data, using_standard_scaler=False):\n",
        "    # VERIFICAR HORIZONTE\n",
        "    input_data = []\n",
        "    output_data = []\n",
        "    for index in range(0, len(data) - janela_tempo):\n",
        "        input_sample = data['radiacao'][index:index + janela_tempo]\n",
        "        output_sample = data['radiacao'][index + janela_tempo]\n",
        "\n",
        "        input_data.append(input_sample)\n",
        "        output_data.append(output_sample)\n",
        "\n",
        "    if using_standard_scaler:\n",
        "        return teste(np.array(input_data)), teste(np.array(output_data))\n",
        "    else:\n",
        "        return np.array(input_data)/1000, np.array(output_data)/1000\n",
        "\n",
        "def data_to_input_and_output_cnn_lstm(df_inpupt, df_target, janela, horizonte=1):\n",
        "    # VERIFICAR HORIZONTE\n",
        "    X_clima, X_rad, y = [], [], []\n",
        "    for i in range(len(df_inpupt) - janela - horizonte + 1):\n",
        "        dados_cnn = df_inpupt.iloc[i:i+janela].values\n",
        "        dados_lstm = df_target.iloc[i:i+janela].values\n",
        "        target = df_target.iloc[i+janela+horizonte-1]\n",
        "\n",
        "        X_clima.append(dados_cnn)\n",
        "        X_rad.append(dados_lstm)\n",
        "        y.append(target)\n",
        "\n",
        "    return np.array(X_clima), np.array(X_rad), np.array(y)\n",
        "\n",
        "def get_files(n=1, lazy=True):\n",
        "    cwd = os.getcwd()\n",
        "\n",
        "    base_path = \"data/\"\n",
        "    dict_data = {}\n",
        "    list_files = [x for x in listdir(base_path) if \".csv\" in x.lower()]\n",
        "\n",
        "    if n:\n",
        "        _range = list_files[0:n]\n",
        "    else:\n",
        "        _range = list_files\n",
        "\n",
        "    for i in _range:\n",
        "        print(\"Processando arquivo \" + i)\n",
        "        code = i.split(\"_\")[3]\n",
        "\n",
        "        cmd = [\"head\", \"-8\", cwd+ \"/\" + base_path + i]\n",
        "        p = subprocess.Popen(cmd, stdout=subprocess.PIPE)\n",
        "\n",
        "        result = p.communicate()\n",
        "        head = result[0].decode('ISO-8859-1')\n",
        "\n",
        "        if lazy:\n",
        "            data = base_path + i\n",
        "        else:\n",
        "            data = pd.read_csv(base_path+i, sep=\";\", skiprows=10)\n",
        "\n",
        "        dict_data[code] = [head, data]\n",
        "\n",
        "    return dict_data\n",
        "\n",
        "def getDictToRenameDataFrame(list_columns):\n",
        "    list_columns_new = []\n",
        "    for i in list_columns:\n",
        "        list_columns_new.append(str\n",
        "              .lower(i)\n",
        "              .replace(\" \", \"_\")\n",
        "              . replace(\"(\", \"_\")\n",
        "              .replace(\")\", \"\")\n",
        "              .replace(\"/\", \"\")\n",
        "              .replace(\"²\", \"2\")\n",
        "              .replace(\"°\", \"\")\n",
        "              .replace(\"%\", \"perc\")\n",
        "              .replace(\"._\", \"_\")\n",
        "              .replace(\".\", \"_\")\n",
        "              .replace(\"__\", \"_\")\n",
        "              .replace(\"_-_\", \"_\")\n",
        "              .replace(\",_\", \"_\")\n",
        "             )\n",
        "\n",
        "    return dict(zip(list_columns, list_columns_new))\n",
        "\n",
        "def cols_standardization(df):\n",
        "    list_columns = df.columns\n",
        "\n",
        "    list_dict_to_rename = getDictToRenameDataFrame(list_columns)\n",
        "\n",
        "    return df.rename(columns=list_dict_to_rename).rename(columns={\"radiacao_kjm2\": \"radiacao\"})\n",
        "\n",
        "def hour_transform(n):\n",
        "    if len(str(n)) == 4:\n",
        "        return str(n)[0:2] + \":\" + str(n)[2:] + \":00\"\n",
        "    elif len(str(n)) == 3:\n",
        "        return \"0\" + str(n)[0:1] + \":\" + str(n)[1:] + \":00\"\n",
        "    elif n == 0:\n",
        "        return \"00:00:00\"\n",
        "\n",
        "def create_datetime_feature(df):\n",
        "    timez = timezone(\"America/Sao_Paulo\")\n",
        "\n",
        "    df[\"hora_medicao\"] = df[\"hora_utc\"].apply(hour_transform)\n",
        "\n",
        "    df[\"data_hora_str\"] = df[\"data\"] + \" \" + df[\"hora_medicao\"].astype(\"str\")\n",
        "\n",
        "    df[\"data_hora\"] = pd.to_datetime(df[\"data_hora_str\"], format=\"%d/%m/%Y %H:%M:%S\")\n",
        "\n",
        "    df = df.set_index(\"data_hora\")\n",
        "    df[\"data_hora\"] = df.index.tz_localize(pytz.utc).tz_convert(timez)\n",
        "    df[\"data\"] = df[\"data_hora\"].dt.date\n",
        "\n",
        "    df[\"data_str\"] = df[\"data\"].astype(\"str\")\n",
        "    df[\"hora\"] = df['data_hora'].dt.hour\n",
        "    return df.drop([\"data_hora_str\", \"hora_utc\", \"hora_medicao\"], axis=1)\n",
        "\n",
        "\n",
        "def create_split_date_features(df):\n",
        "    df[\"dia\"] = df[\"data_hora\"].dt.day\n",
        "    df[\"mes\"] = df[\"data_hora\"].dt.month\n",
        "    df[\"ano\"] = df[\"data_hora\"].dt.year\n",
        "\n",
        "    return df\n",
        "\n",
        "def create_category(column, df):\n",
        "    labels = [\"A\", \"B\", \"C\", \"D\"]\n",
        "    classes = df.describe()[column][3:8].values\n",
        "\n",
        "    if classes[1] == 0:\n",
        "        classes[1] = classes[1]+0.1\n",
        "        print(\"aqui\")\n",
        "    print(classes)\n",
        "\n",
        "    return pd.cut(x = df[column],\n",
        "         bins = classes,\n",
        "         labels = labels,\n",
        "         include_lowest = True)\n",
        "\n",
        "def removeNulls(df, col):\n",
        "    return df[df[col].notnull()]\n",
        "\n",
        "def pre_processing(raw_df, is_brt=False):\n",
        "    df = cols_standardization(raw_df)\n",
        "    df = create_datetime_feature(df)\n",
        "    df = create_split_date_features(df)\n",
        "    df = removeNulls(df, \"radiacao\")\n",
        "    df = change_types(df)\n",
        "\n",
        "    return df\n",
        "\n",
        "def reduce_df(list_df, key):\n",
        "    if not key:\n",
        "        return reduce(lambda x, y: pd.merge(x, y), list_df)\n",
        "\n",
        "    return reduce(lambda x, y: pd.merge(x, y, on = key), list_df)\n",
        "\n",
        "def load(path):\n",
        "    return pd.read_csv(path, sep=\";\", encoding = \"ISO-8859-1\", skiprows=8)\n",
        "\n",
        "def get_perc_nulls(df):\n",
        "    return (df.isnull().sum()/(len(df)))*100\n",
        "\n",
        "def get_percentils(df, col):\n",
        "    for i in range(0, 101):\n",
        "        value_str = str(i)\n",
        "\n",
        "        if len(value_str) == 1:\n",
        "            value_str = \"0.0\"+value_str\n",
        "\n",
        "        elif len(value_str) == 2:\n",
        "            value_str = \"0.\"+value_str\n",
        "        else:\n",
        "            value_str = \"1.0\"\n",
        "\n",
        "        double_value = float(value_str)\n",
        "        print(value_str, df[col].quantile(double_value))\n",
        "\n",
        "def plot_by_col(df, col_grouped, col_target, axs, estacao=None):\n",
        "    df = df[[col_grouped, col_target]].groupby([col_grouped]).mean().reset_index()\n",
        "\n",
        "    if axs is None:\n",
        "      sns.barplot(data=df, x=df[col_grouped], y=df[col_target])\n",
        "\n",
        "    else:\n",
        "      axs.set_ylabel(str(estacao))\n",
        "      sns.barplot(data=df, x=df[col_grouped], y=df[col_target], ax=axs)\n",
        "\n",
        "\n",
        "def plot_by_range(df, col_x, col_y, dt_start, dt_end):\n",
        "    df = filter_between(df, col_x, dt_start, dt_end)\n",
        "\n",
        "    sns.lineplot(data=df, x=df[col_x], y=df[col_y])\n",
        "\n",
        "def filter_between(df, col, value_1, value_2):\n",
        "    _filter = (df[col] >= value_1) & (df[col] <= value_2)\n",
        "    return df[_filter]\n",
        "\n",
        "def set_plot_size(x, y):\n",
        "    sns.set_theme(rc={'figure.figsize':(x,y)})\n",
        "\n",
        "def change_types(df):\n",
        "    list_columns = df.drop(\"data_str\", axis=1).columns\n",
        "\n",
        "    for i in list_columns:\n",
        "        if 'object' in df[i].dtypes.name:\n",
        "            try:\n",
        "                df[i] = df[i].str.replace(\",\", \".\").astype('float64')\n",
        "            except:\n",
        "                continue\n",
        "\n",
        "        else:\n",
        "            continue\n",
        "\n",
        "\n",
        "    return df\n",
        "\n",
        "def treat_columns(df):\n",
        "\n",
        "    columns_to_drop = ['pressão_atmosferica_max_na_hora_ant_aut_mb',\n",
        "                       'pressão_atmosferica_min_na_hora_ant_aut_mb',\n",
        "                       'temperatura_do_ponto_de_orvalho_c',\n",
        "                       'temperatura_máxima_na_hora_ant_aut_c',\n",
        "                       'temperatura_mínima_na_hora_ant_aut_c',\n",
        "                       'temperatura_orvalho_max_na_hora_ant_aut_c',\n",
        "                       'temperatura_orvalho_min_na_hora_ant_aut_c',\n",
        "                       'umidade_rel_max_na_hora_ant_aut_perc',\n",
        "                       'umidade_rel_min_na_hora_ant_aut_perc',\n",
        "                      'vento_direção_horaria_gr__gr',\n",
        "                      'vento_rajada_maxima_ms']\n",
        "\n",
        "    columns_to_rename = {'precipitação_total_horário_mm': 'precipitacao',\n",
        "                        'pressao_atmosferica_ao_nivel_da_estacao_horaria_mb': 'press_atmo',\n",
        "                        'temperatura_do_ar_bulbo_seco_horaria_c': 'temperatura',\n",
        "                        'umidade_relativa_do_ar_horaria_perc': 'umidade',\n",
        "                         'vento_velocidade_horaria_ms':'vento_velocidade_horaria'}\n",
        "\n",
        "    return df.rename(columns=columns_to_rename).drop(columns_to_drop, axis=1)\n",
        "\n",
        "def my_autocov(df, col, interval=1):\n",
        "    serie = np.array(df[col].to_list())\n",
        "\n",
        "    # serie = np.array([0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20])\n",
        "\n",
        "    xt = serie[:-interval]\n",
        "    xt1 = serie[interval:]\n",
        "    n_pairs = len(xt)\n",
        "\n",
        "    mean_xt = xt.sum(axis=0)/n_pairs\n",
        "    mean_xt1 = xt1.sum(axis=0)/n_pairs\n",
        "\n",
        "    dev_xt = xt - mean_xt\n",
        "    dev_xt1 = xt1 - mean_xt1\n",
        "\n",
        "    d = {\"xt\": xt, \"xt1\": xt1}\n",
        "    df_new = pd.DataFrame(data=d)\n",
        "\n",
        "    sns.scatterplot(df_new, x=\"xt\", y=\"xt1\")\n",
        "\n",
        "    return dev_xt.dot(dev_xt1)/n_pairs\n",
        "\n",
        "def plot_distrib_horario(df, axs, estacao=None):\n",
        "    local_anual_df = df[[\"data_hora\", \"radiacao\", \"temp_ins_c\"]]\n",
        "    local_anual_df[\"hora\"] = local_anual_df[\"data_hora\"].dt.hour\n",
        "\n",
        "    plot_by_col(local_anual_df, \"hora\", \"radiacao\", axs, estacao)\n",
        "\n",
        "def plot_results(df_orig, pred, real):\n",
        "    set_plot_size(12, 6)\n",
        "\n",
        "    period = df_orig[0:len(df_orig) - janela_tempo]['data']\n",
        "    pred = pred.reshape(pred.shape[0])\n",
        "\n",
        "    df_pred = pd.DataFrame(data=pred, index=period, columns=[\"predicted\"])\n",
        "    df_real = pd.DataFrame(data=real, index=period, columns=[\"real\"])\n",
        "\n",
        "    sns.lineplot(df_pred, palette=[\"red\"], ci=None)\n",
        "    sns.lineplot(df_real, palette=[\"blue\"], ci=None)\n",
        "\n",
        "def plot_results_2(pred, real, period):\n",
        "    set_plot_size(12, 6)\n",
        "\n",
        "    pred = pred.reshape(pred.shape[0])\n",
        "\n",
        "    df_pred = pd.DataFrame(data=pred, index=period, columns=[\"predicted\"])\n",
        "    df_real = pd.DataFrame(data=real, index=period, columns=[\"real\"])\n",
        "\n",
        "    sns.lineplot(df_pred, palette=[\"red\"], ci=None)\n",
        "    sns.lineplot(df_real, palette=[\"blue\"], ci=None)\n",
        "\n",
        "def get_metrics(test_pred, y_val):\n",
        "    mse = mean_squared_error(test_pred.reshape(test_pred.shape[0]), y_val)\n",
        "    rmse = math.sqrt(mse)\n",
        "    mae = mean_absolute_error(test_pred.reshape(test_pred.shape[0]), y_val)\n",
        "    r2 = r2_score(test_pred.reshape(test_pred.shape[0]), y_val)\n",
        "\n",
        "    return {\"mse\": mse, \"rmse\": rmse, \"mae\": mae, \"r2\": r2}\n",
        "\n",
        "def normalize_leo(X):\n",
        "    scaler = MinMaxScaler()\n",
        "    return scaler.fit_transform(X)\n",
        "\n",
        "def teste(values):\n",
        "    scaler = StandardScaler()\n",
        "    if len(values.shape) == 1:\n",
        "        values = values.reshape(-1, 1)\n",
        "\n",
        "    scaler = scaler.fit(values)\n",
        "    # print('Mean: %f, StandardDeviation: %f' % (scaler.mean_, math.sqrt(scaler.var_)))\n",
        "    # normalize the dataset and print\n",
        "    standardized = scaler.transform(values)\n",
        "\n",
        "    return standardized\n",
        "\n",
        "def integrated_gradients(model,\n",
        "                          input_tensor,\n",
        "                          target_class_idx=None,\n",
        "                          baseline=None,\n",
        "                          m_steps=100):\n",
        "    \"\"\"\n",
        "    Compute Integrated Gradients for a Keras/TensorFlow model.\n",
        "    \"\"\"\n",
        "    if baseline is None:\n",
        "        baseline = np.zeros_like(input_tensor).astype(np.float32)\n",
        "\n",
        "    input_tensor = input_tensor.astype(np.float32)\n",
        "    baseline = baseline.astype(np.float32)\n",
        "\n",
        "    interpolated_inputs = [\n",
        "        baseline + (float(alpha) / m_steps) * (input_tensor - baseline)\n",
        "        for alpha in range(0, m_steps + 1)\n",
        "    ]\n",
        "    interpolated_inputs = tf.convert_to_tensor(np.concatenate(interpolated_inputs, axis=0))\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        tape.watch(interpolated_inputs)\n",
        "        preds = model(interpolated_inputs)\n",
        "\n",
        "        if target_class_idx is not None:\n",
        "            outputs = preds[:, target_class_idx]\n",
        "        else:\n",
        "            outputs = tf.reduce_sum(preds, axis=1)  # Regressão\n",
        "\n",
        "    grads = tape.gradient(outputs, interpolated_inputs)\n",
        "\n",
        "    avg_grads = tf.reduce_mean(grads[:-1], axis=0)  # Exclui último ponto\n",
        "\n",
        "    integrated_grads = (input_tensor - baseline) * avg_grads.numpy()\n",
        "\n",
        "    return integrated_grads.squeeze()\n",
        "\n",
        "def get_feature_importance(X_train, Y_train, cols, n_features):\n",
        "\n",
        "    time_steps = 11\n",
        "    # n_features = 16\n",
        "\n",
        "    model = Sequential([\n",
        "        Input(shape=(n_features, 1)),\n",
        "        Conv1D(32, kernel_size=2, activation='relu'),\n",
        "        Conv1D(16, kernel_size=2, activation='relu'),\n",
        "        GlobalAveragePooling1D(),\n",
        "        Dense(1, activation='linear')  # Para regressão\n",
        "    ])\n",
        "\n",
        "    model.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "    model.fit(X_train, Y_train, epochs=10, verbose=0)\n",
        "\n",
        "    X_sample = X_train[0:1]\n",
        "\n",
        "    attributions = integrated_gradients(model, X_sample, m_steps=100)\n",
        "\n",
        "\n",
        "    d = {\"feature\": cols, \"value\": attributions}\n",
        "\n",
        "    local_df = pd.DataFrame(data=d)\n",
        "    plt.xticks(rotation=90)\n",
        "    sns.barplot(local_df, x=\"feature\", y=\"value\")\n",
        "\n",
        "def create_table_results(station, nn_type, results):\n",
        "\n",
        "    cols = list(results[0][\"metrics\"].keys())\n",
        "\n",
        "    list_metrics = np.transpose([[results[x][\"metrics\"][y] for x in results] for y in cols])\n",
        "\n",
        "    index_times = list_metrics.shape[0]\n",
        "\n",
        "    df = pd.DataFrame(\n",
        "        list_metrics,\n",
        "        columns=[cols]\n",
        "    )\n",
        "\n",
        "    arrays = [\n",
        "        [station for x in range(index_times)],\n",
        "        [nn_type for x in range(index_times)],\n",
        "        [str(x+1) for x in range(index_times)]]\n",
        "\n",
        "    tuples = list(zip(*arrays))\n",
        "\n",
        "    index = pd.MultiIndex.from_tuples(tuples, names=[\"estacao\", \"tipo_rede\", \"execucao\"])\n",
        "\n",
        "    return df.set_index(index)\n",
        "\n",
        "def create_table_results2(station, results):\n",
        "    cols = list(results.keys())\n",
        "\n",
        "    list_metrics = np.transpose([[results[y]] for y in cols])\n",
        "    df = pd.DataFrame(\n",
        "          list_metrics,\n",
        "          columns=[cols]\n",
        "      )\n",
        "\n",
        "    df[\"station\"] = station\n",
        "\n",
        "    return df\n",
        "\n",
        "def adfuller_test(df):\n",
        "    result=ts.adfuller(df)\n",
        "    labels = ['Teste estatístico ADF','p-valor','Num Lags','Numero de observações']\n",
        "    for value, label in zip(result,labels):\n",
        "        print(label+' : '+str(value) )\n",
        "    if result[1] <= 0.05:\n",
        "        print(\"Fortes evidências contra a hipotese nula(Ho), ou seja, pode ser rejeitada. Os dados não possuem uma raíz unitária, portanto tem estacionáriedade.\")\n",
        "    else:\n",
        "        print(\"Fracas evidências contra a hipotese nula(Ho), a série temporal possui uma raíz unitária, indicando que é não estacionária.\")\n",
        "\n",
        "\n",
        "def get_seasonal_interval(df, start_hour):\n",
        "    serie = df[\"hora\"]\n",
        "    indices = serie[serie == start_hour].index.to_list()\n",
        "\n",
        "    if len(indices) < 2:\n",
        "        print(\"O valor aparece menos de duas vezes.\")\n",
        "        return None\n",
        "\n",
        "    indices_posicionais = [serie.index.get_loc(i) for i in indices]\n",
        "\n",
        "    return indices_posicionais[1]\n",
        "\n",
        "def calculate_acf_pacf(df):\n",
        "    df_acf_pacf = df[[\"data_hora\", \"radiacao\"]].set_index(\"data_hora\")\n",
        "\n",
        "    fig = plt.figure(figsize=(12,8))\n",
        "    ax1 = fig.add_subplot(211)\n",
        "    fig = sm.graphics.tsa.plot_acf(df_acf_pacf,lags=40, ax=ax1)\n",
        "    ax2 = fig.add_subplot(212)\n",
        "    fig = sm.graphics.tsa.plot_pacf(df_acf_pacf,lags=40, ax=ax2)\n",
        "\n",
        "def filter_seasons(df, date_col):\n",
        "    outono = ((df[date_col] >= \"2023-03-20\") & (df[date_col] < \"2023-06-20\")) | ((df[date_col] >= \"2024-03-20\") & (df[date_col] < \"2024-06-20\"))\n",
        "    inverno = ((df[date_col] >= \"2023-06-20\") & (df[date_col] < \"2023-09-22\")) | ((df[date_col] >= \"2024-06-20\") & (df[date_col] < \"2024-09-22\"))\n",
        "    primavera = ((df[date_col] >= \"2023-09-22\") & (df[date_col] < \"2023-12-21\")) | ((df[date_col] >= \"2024-09-22\") & (df[date_col] < \"2024-12-21\"))\n",
        "    verao = ((df[date_col] >= \"2023-12-21\") & (df[date_col] < \"2024-03-20\")) | ((df[date_col] >= \"2024-12-21\") & (df[date_col] < \"2025-03-20\"))\n",
        "\n",
        "    return df[outono], df[inverno], df[primavera], df[verao]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "14ae699b-f0c0-4b87-8ffa-be66c07a7430",
      "metadata": {
        "id": "14ae699b-f0c0-4b87-8ffa-be66c07a7430"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}